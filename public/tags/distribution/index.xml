<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Distribution on Leo&#39;s sharing space</title>
        <link>https://example.org/tags/distribution/</link>
        <description>Recent content in Distribution on Leo&#39;s sharing space</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 11 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/tags/distribution/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Distributions in Statistics</title>
        <link>https://example.org/post/distributions/</link>
        <pubDate>Wed, 11 Jun 2025 00:00:00 +0000</pubDate>
        
        <guid>https://example.org/post/distributions/</guid>
        <description>&lt;img src="https://example.org/post/distributions/Minecraft_building.png" alt="Featured image of post Distributions in Statistics" /&gt;&lt;p&gt;There are numerous distributions in statistics. The most famous one would be the norm distribution (Gaussian distribution). In this note, I will introduce them one by one, explain their characteristics, and their applications in data analysis.&lt;/p&gt;
&lt;h1 id=&#34;indicator-random-variable&#34;&gt;Indicator random variable
&lt;/h1&gt;&lt;p&gt;&lt;em&gt;Definition&lt;/em&gt; The indicator random variable of an event $A$ is the random variable which equals 1 if $A$ occurs and 0 otherwise. We will denote the indicator random variable of $A$ by $I_A$ or $I(A)$.&lt;/p&gt;
&lt;h1 id=&#34;bernouli-distribution&#34;&gt;Bernouli distribution
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (Bernouli distribution) An random variable $X$ is said to have the &lt;em&gt;Bernouli distribution&lt;/em&gt; with parameter $p$ if $P(X = 1) = p$ and $P(X = 0) = 1-p$, where $0&amp;lt;P&amp;lt;1$. We write this as $X ~ Bern(p)$.&lt;/p&gt;
&lt;h1 id=&#34;binomial-distribution&#34;&gt;Binomial distribution
&lt;/h1&gt;&lt;p&gt;Suppose there are $n$ indepedent Bernouli trails are performed, each with same success probability $p$. Let $X$ be the number of successes. The distribution of $X$ is called the &lt;em&gt;Bionomial distribution&lt;/em&gt; with parameters $n$ and $p$. We write $X \sim Bin(n,p)$ to mean that $X$ has the Bionomial distribution with parameters $n$ and $p$, where $n$ is a positive integer and $0&amp;lt;p&amp;lt;1$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Binomial PMF). If $X \sim Bim(n,p)$, then the PMF of $X$ is
$$P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$$
for $k=0,1,&amp;hellip;,n$ (and $P(X=k)=0$ otherwise).&lt;/p&gt;
&lt;h1 id=&#34;hypergeometric-distribution&#34;&gt;Hypergeometric distribution
&lt;/h1&gt;&lt;p&gt;Consider an urn with $w$ white balls and $b$ black balls. We draw $n$ balls out of the urn at random without replacement, such that all $\binom{w+b}{n}$ samples are equally likely. Let $X$ be the number of white balls in the sample. Then $X$ is said to have the &lt;em&gt;Hypergeometric distribution&lt;/em&gt; with paramters $w$, $b$, and $n$; we denote this by $X \sim HGeom(w,b,n)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Hypergenometric PMF) If $X \sim HGeom(w,b,n)$, then the PMF of $X$ is
$$P(X=k)=\frac{\binom{w}{k}\binom{b}{n-k}}{\binom{w+b}{n}}$$
for integers $k$ satisfying $0 \le k \le w$ and $0 \le n-k \le b$, and $P(X=k)=0$ otherwise.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Beta distribution</title>
        <link>https://example.org/post/beta_distribution/</link>
        <pubDate>Wed, 14 May 2025 15:10:48 +0800</pubDate>
        
        <guid>https://example.org/post/beta_distribution/</guid>
        <description>&lt;img src="https://example.org/post/beta_distribution/logcabin.jpg" alt="Featured image of post Beta distribution" /&gt;&lt;h2 id=&#34;beta-distribution&#34;&gt;Beta distribution
&lt;/h2&gt;&lt;p&gt;The Beta distribution is a continuous distribution on the interval (0,1). It is a generalization of the Unif(0,1) distribution.&lt;/p&gt;
&lt;p&gt;An random variable $X$ is said to have the Beta distribution with parameters $a$ and $b$, where $a,b &amp;gt; 0$, if its probability density function (PDF) is:&lt;/p&gt;
&lt;p&gt;$$f(x)=\frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1},\quad0&amp;lt;x&amp;lt;1$$&lt;/p&gt;
&lt;p&gt;$\beta(a,b)$ is the beta function. It is a constant that chosen to make the PDF integrate to 1. We write this as $X \sim \text{Beta}(a,b)$.&lt;/p&gt;
&lt;p&gt;Taking $a=b=1$, the $Beta(1,1)$ PDF is $f(x)=1$, which is same as the uniform distribution $Unif(0,1)$, the proof is shown below.&lt;/p&gt;
&lt;h2 id=&#34;beta-function&#34;&gt;Beta function
&lt;/h2&gt;&lt;p&gt;To make the PDF of Beta distribution integrate to 1. The constant $\beta(a,b)$ is defined as:&lt;/p&gt;
&lt;p&gt;$$\beta(a,b)=\int_0^1 x^{a-1}(1-x)^{b-1}dx=\frac{(a-1)!(b-1)!}{(a+b-1)!}$$&lt;/p&gt;
&lt;p&gt;$Proof.$
Let&amp;rsquo;s use the induction to prove the above equation.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Base Case:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For $b=1$:
$$\beta(a,1)=\int_0^1x^{a-1}dx=\frac{1}{a}=\frac{(a-1)!(1-1)!}{(a+1-1)!}$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Recurrence Relation of the Beta Function:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using integration by parts on $\beta(a,b)$:
$$\beta(a,b+1)=\frac{b}{a+b}\beta(a,b)$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Inductive Step:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s assume the formula holds for $b=k$, i.e., $\beta(a,k)=\frac{(a-1)!(k-1)!}{(a+k-1)!}$.&lt;/p&gt;
&lt;p&gt;Then we have:
$$\beta(a,k+1)=\int_0^1 x^{a-1}(1-x)^{k}dx=\int_0^1 x^{a-1}(1-x)^{k}dx$$
As shown in 2, we can write:
$$\beta(a,k+1)=\frac{k}{a+k}\int_0^1 x^{a-1}(1-x)^{k-1}dx$$
$$=\frac{k}{a+k}\cdot\frac{(a-1)!(k-1)!}{(a+k-1)!}$$
$$=\frac{(a-1)!(k)!}{(a+k)!}$$
Thus, we have shown that if the formula holds for $b=k$, it also holds for $b=k+1$.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the principle of mathematical induction, the formula holds for all positive integers $b$. Symmetrically, we can also show that the formula holds for $a$ by using the same induction process.&lt;/p&gt;
&lt;h2 id=&#34;beta-binomial-conjugacy&#34;&gt;Beta-Binomial conjugacy
&lt;/h2&gt;&lt;p&gt;The Beta distribution is the conjugate prior of the Binomial distribution. This means that if we have a Binomial likelihood and a Beta prior, the posterior distribution will also be a Beta distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we have a coin that we want to test for bias. We flip the coin $n$ times and observe $k$ heads. We can model this with a Binomial distribution, where the probability of heads is $p$. The likelihood function is given by:
$$P(X=k|p)=\binom{n}{k}p^k(1-p)^{n-k}$$
where $X$ is the number of heads observed in $n$ flips.
The likelihood function is a function of $p$, and it tells us how likely we are to observe $k$ heads given a particular value of $p$.&lt;/p&gt;
&lt;p&gt;For example, if we have a Binomial likelihood with parameters $n$ and $k$, and a Beta prior with parameters $a$ and $b$, the posterior distribution will be:&lt;/p&gt;
&lt;p&gt;$$f(p|X=k)=\frac{P(X=k|p)f(p)}{P(X=k)}=\frac{(_k^n)p^k(1-p)^{n-k}\cdot\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)}$$
$$=\frac{(_k^n)p^k(1-p)^{n-k}\cdot\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{\int_0^1P(X=k|p)f(p)dp}$$&lt;/p&gt;
&lt;p&gt;It is difficult to calculate the denominator. Are we stucked here?&lt;/p&gt;
&lt;p&gt;Acutally, the calculation is much easier than it appears. The conditional PDF $f(p|X=k)$ is a function of $p$, which means everthing that desn&amp;rsquo;t depend on $p$ is a constant. We can drop all these constants and find the PDF up to a multiplicative constant (and then the normalizing constant is whatever it needs to make the PDF integrate to 1). We can drop the $(_k^n)$, $\frac{1}{\beta(a,b)},$ and the denominator $P(X=k)$. So we can write:&lt;/p&gt;
&lt;p&gt;$$f(p|X=k)\propto p^{k+a-1}(1-p)^{n-k+b-1}$$&lt;/p&gt;
&lt;p&gt;Which is very close to the PDF of a Beta distribution $Beta(a+k, b+n-k)$, up to a multiplicative constant. Therefore, we can conclude that the posterior distribution is:&lt;/p&gt;
&lt;p&gt;$$f(p|X=k)\sim Beta(a+k, b+n-k)$$&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
