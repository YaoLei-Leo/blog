[{"content":"There are numerous distributions in statistics. The most famous one would be the norm distribution (Gaussian distribution). In this note, I will introduce them one by one, explain their characteristics, and their applications in data analysis.\nIndicator random variable\rDefinition The indicator random variable of an event $A$ is the random variable which equals 1 if $A$ occurs and 0 otherwise. We will denote the indicator random variable of $A$ by $I_A$ or $I(A)$.\nBernouli distribution\rDefinition (Bernouli distribution) An random variable $X$ is said to have the Bernouli distribution with parameter $p$ if $P(X = 1) = p$ and $P(X = 0) = 1-p$, where $0\u0026lt;P\u0026lt;1$. We write this as $X ~ Bern(p)$.\nBinomial distribution\rSuppose there are $n$ indepedent Bernouli trails are performed, each with same success probability $p$. Let $X$ be the number of successes. The distribution of $X$ is called the Bionomial distribution with parameters $n$ and $p$. We write $X \\sim Bin(n,p)$ to mean that $X$ has the Bionomial distribution with parameters $n$ and $p$, where $n$ is a positive integer and $0\u0026lt;p\u0026lt;1$.\nTheorem (Binomial PMF). If $X \\sim Bim(n,p)$, then the PMF of $X$ is $$P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}$$ for $k=0,1,\u0026hellip;,n$ (and $P(X=k)=0$ otherwise).\nHypergeometric distribution\rConsider an urn with $w$ white balls and $b$ black balls. We draw $n$ balls out of the urn at random without replacement, such that all $\\binom{w+b}{n}$ samples are equally likely. Let $X$ be the number of white balls in the sample. Then $X$ is said to have the Hypergeometric distribution with paramters $w$, $b$, and $n$; we denote this by $X \\sim HGeom(w,b,n)$.\nTheorem (Hypergenometric PMF) If $X \\sim HGeom(w,b,n)$, then the PMF of $X$ is $$P(X=k)=\\frac{\\binom{w}{k}\\binom{b}{n-k}}{\\binom{w+b}{n}}$$ for integers $k$ satisfying $0 \\le k \\le w$ and $0 \\le n-k \\le b$, and $P(X=k)=0$ otherwise.\n","date":"2025-06-11T00:00:00Z","image":"https://example.org/post/distributions/Minecraft_building_hu334960280979990242.png","permalink":"https://example.org/post/distributions/","title":"Distributions in Statistics"},{"content":"Law of total probability\rLet\u0026rsquo;s say we have a set of mutually exclusive probable events ${B_i : i=1,2,3,\u0026hellip;,n }$. The sum of the probabilities of these events is equal to 1, i.e. $$\\sum_i P(B_i)=1$$\nNow, if we have another event $A$, we can express the probability of $A$ in terms of the probabilities of $B_i$ and the conditional probabilities of $A$ given $B_i$. This is known as the law of total probability: $$P(A)=\\sum_i P(A|B_i)P(B_i)$$\nHere, as the sum of the probabilities of $B_i$ is equal to 1. The event $A$ must happen in at least one of the $B_i$ events, or the $P(A)$ is equal to 0. We can understand this as the probability of $A$ is the sum of the probabilities of $A$ happening in each of the mutually exclusive events $B_i$.\nBayes\u0026rsquo; rule\rFor any specific event $B_j$, the probability of both event $A$ and $B_j$ happending is $$P(A,B_j)=P(A|B_j)P(B_j)$$\nAlso, we can express the same probability in terms of the conditional probability of $B_j$ given $A$: $$P(A,B_j)=P(B_j|A)P(A)$$\nTherefore, we can write get this formula: $$P(A,B_j)=P(A|B_j)P(B_j)=P(B_j|A)P(A)$$\nHere comes the Bayes\u0026rsquo; rule: $$P(A|B_j)=\\frac{P(B_j|A)P(A)}{P(B_j)} \\tag{1}$$ $$or$$ $$P(B_j|A)=\\frac{P(A|B_j)P(B_j)}{P(A)}$$\nUsing the formula (1) as example, we call the $P(A)$ as the prior probability of event $A$ happening. The $P(A|B_j)$ is the posterior probability of event $A$ given that event $B_j$ has happened. The $P(B_j|A)$ is the likelihood of event $B_j$ given that event $A$ has happened. The $P(B_j)$ is the marginal probability of event $B_j$.\nBayesian inference\rBayesian inference is a method of statistical inference in which Bayes\u0026rsquo; theorem is used to update the probability for a hypothesis as more evidence or information becomes available. In Bayesian inference, we start with a prior distribution, which represents our beliefs about the parameters before observing any data. After observing the data, we update our beliefs using Bayes\u0026rsquo; rule to obtain the posterior distribution.\nUsing the formula (1) as example, $P(A)$ is the prior distribution of event $A$, which represents our beliefs about the parameters before observing any data. The $P(B_j|A)$ is the likelihood of event $B_j$ given that event $A$ has happened. The $P(B_j)$ is the marginal distribution of event $B_j$. We update our beliefs about the $P(A)$ by observing the data $B_j$ and using Bayes\u0026rsquo; rule to get the posterior probability distribution $P(A|B_j)$.\nThere are many types of prior distributions, and also many types of likelihood functions. The choice of prior distribution and likelihood function depends on the problem at hand and the available data. Some combination of prior distribution and likelihood function can lead to a closed-form solution, which means we can derive the exact formula for the posterior distribution, while others may require numerical methods such as Markov Chain Monte Carlo (MCMC) to approximate the posterior distribution. Here, we first introduce the closed-form cases, and then introduce the numerical methods.\nClosed-form solutions: conjugate priors\rBeta prior and binomial likelihood\rThe Beta distribution is a conjugate prior for the binomial likelihood. If we have a binomial likelihood $P(X=k|p)=\\binom{n}{k}p^k(1-p)^{n-k}$, where $X$ is the number of successes in $n$ trials and $p$ is the probability of success, we can use a Beta prior $P(p|\\alpha,\\beta)=\\frac{1}{B(\\alpha,\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}$, where $\\alpha$ and $\\beta$ are the shape parameters of the Beta distribution.\nUsing Bayes\u0026rsquo; rule, we can derive the posterior distribution: $$P(p|X=k)=\\frac{P(X=k|p)P(p)}{P(X=k)}$$ Substituting the binomial likelihood and Beta prior, we get: $$P(p|X=k)=\\frac{\\binom{n}{k}p^k(1-p)^{n-k}\\frac{1}{B(\\alpha,\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}}{P(X=k)} \\tag{2}$$\nThe numerator of the equation (2) can be simplified as: $$P(p|X=k)=\\frac{1}{P(X=k)}\\cdot\\frac{1}{B(\\alpha,\\beta)}\\binom{n}{k}p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}$$\nHere, the posterior distribution is a function of $p$. The constant $\\frac{1}{B(\\alpha,\\beta)}$, $(_k^n)$, and the $P(X=k)$ are all constants with respect to $p$. Therefore, we can first ignore them when we are only interested in the shape of the posterior distribution. Therefore, we can write the posterior distribution as: $$P(p|X=k) \\propto p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1} \\tag{3}$$\nHere comes the magic part, the posterior distribution is similar to the PDF of Beta distribution, with the shape parameters $\\alpha\u0026rsquo; = k + \\alpha$ and $\\beta\u0026rsquo; = n - k + \\beta$: $$\\beta(k+\\alpha, n-k+\\beta)=\\frac{1}{B(k+\\alpha,n-k+\\beta)}p^{k+\\alpha-1}(1-p)^{n-k+\\beta}$$\nBut missing the Beta function $\\frac{1}{B(k+\\alpha,n-k+\\beta)}$. Interestly, the formula (3) tells us that the posterior distribution is proportional to the $p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}$, the constant parts are ignored. In fact, the true posterior distribution should be: $$P(p|X=k) = constant \\cdot p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}$$\nTo get the constant. We can use the characteristics of a PDF that the integral of the PDF is equal to 1: $$\\int_0^1 P(p|X=k)dp = 1$$ This integral can be solved as: $$\\int_0^1 p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}dp = B(k+\\alpha, n-k+\\beta)$$ Therefore, we can get the constant: $$constant = \\frac{1}{B(k+\\alpha, n-k+\\beta)}$$ Thus, the posterior distribution is: $$P(p|X=k) = \\frac{1}{B(k+\\alpha, n-k+\\beta)}p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}$$\nNow, we can see that the posterior distribution is a Beta distribution with shape parameters $\\alpha\u0026rsquo; = k + \\alpha$ and $\\beta\u0026rsquo; = n - k + \\beta$. This is the closed-form solution for the posterior distribution when using a Beta prior and a binomial likelihood. And this is the reason why we call the Beta distribution a conjugate prior for the binomial likelihood.\nNormal prior and normal likelihood\r","date":"2025-05-24T00:00:00Z","image":"https://example.org/post/bayes_theory/Minecraft_view_hu4640479011387587021.jpg","permalink":"https://example.org/post/bayes_theory/","title":"Bayes' Theory and Bayesian Inference"},{"content":"Beta distribution\rThe Beta distribution is a continuous distribution on the interval (0,1). It is a generalization of the Unif(0,1) distribution.\nAn random variable $X$ is said to have the Beta distribution with parameters $a$ and $b$, where $a,b \u0026gt; 0$, if its probability density function (PDF) is:\n$$f(x)=\\frac{1}{\\beta(a,b)}x^{a-1}(1-x)^{b-1},\\quad0\u0026lt;x\u0026lt;1$$\n$\\beta(a,b)$ is the beta function. It is a constant that chosen to make the PDF integrate to 1. We write this as $X \\sim \\text{Beta}(a,b)$.\nTaking $a=b=1$, the $Beta(1,1)$ PDF is $f(x)=1$, which is same as the uniform distribution $Unif(0,1)$, the proof is shown below.\nBeta function\rTo make the PDF of Beta distribution integrate to 1. The constant $\\beta(a,b)$ is defined as:\n$$\\beta(a,b)=\\int_0^1 x^{a-1}(1-x)^{b-1}dx=\\frac{(a-1)!(b-1)!}{(a+b-1)!}$$\n$Proof.$ Let\u0026rsquo;s use the induction to prove the above equation.\nBase Case: For $b=1$: $$\\beta(a,1)=\\int_0^1x^{a-1}dx=\\frac{1}{a}=\\frac{(a-1)!(1-1)!}{(a+1-1)!}$$\nRecurrence Relation of the Beta Function: Using integration by parts on $\\beta(a,b)$: $$\\beta(a,b+1)=\\frac{b}{a+b}\\beta(a,b)$$\nInductive Step: Let\u0026rsquo;s assume the formula holds for $b=k$, i.e., $\\beta(a,k)=\\frac{(a-1)!(k-1)!}{(a+k-1)!}$.\nThen we have: $$\\beta(a,k+1)=\\int_0^1 x^{a-1}(1-x)^{k}dx=\\int_0^1 x^{a-1}(1-x)^{k}dx$$ As shown in 2, we can write: $$\\beta(a,k+1)=\\frac{k}{a+k}\\int_0^1 x^{a-1}(1-x)^{k-1}dx$$ $$=\\frac{k}{a+k}\\cdot\\frac{(a-1)!(k-1)!}{(a+k-1)!}$$ $$=\\frac{(a-1)!(k)!}{(a+k)!}$$ Thus, we have shown that if the formula holds for $b=k$, it also holds for $b=k+1$.\nConclusion: By the principle of mathematical induction, the formula holds for all positive integers $b$. Symmetrically, we can also show that the formula holds for $a$ by using the same induction process.\nBeta-Binomial conjugacy\rThe Beta distribution is the conjugate prior of the Binomial distribution. This means that if we have a Binomial likelihood and a Beta prior, the posterior distribution will also be a Beta distribution.\nLet\u0026rsquo;s say we have a coin that we want to test for bias. We flip the coin $n$ times and observe $k$ heads. We can model this with a Binomial distribution, where the probability of heads is $p$. The likelihood function is given by: $$P(X=k|p)=\\binom{n}{k}p^k(1-p)^{n-k}$$ where $X$ is the number of heads observed in $n$ flips. The likelihood function is a function of $p$, and it tells us how likely we are to observe $k$ heads given a particular value of $p$.\nFor example, if we have a Binomial likelihood with parameters $n$ and $k$, and a Beta prior with parameters $a$ and $b$, the posterior distribution will be:\n$$f(p|X=k)=\\frac{P(X=k|p)f(p)}{P(X=k)}=\\frac{(_k^n)p^k(1-p)^{n-k}\\cdot\\frac{1}{\\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)}$$ $$=\\frac{(_k^n)p^k(1-p)^{n-k}\\cdot\\frac{1}{\\beta(a,b)}p^{a-1}(1-p)^{b-1}}{\\int_0^1P(X=k|p)f(p)dp}$$\nIt is difficult to calculate the denominator. Are we stucked here?\nAcutally, the calculation is much easier than it appears. The conditional PDF $f(p|X=k)$ is a function of $p$, which means everthing that desn\u0026rsquo;t depend on $p$ is a constant. We can drop all these constants and find the PDF up to a multiplicative constant (and then the normalizing constant is whatever it needs to make the PDF integrate to 1). We can drop the $(_k^n)$, $\\frac{1}{\\beta(a,b)},$ and the denominator $P(X=k)$. So we can write:\n$$f(p|X=k)\\propto p^{k+a-1}(1-p)^{n-k+b-1}$$\nWhich is very close to the PDF of a Beta distribution $Beta(a+k, b+n-k)$, up to a multiplicative constant. Therefore, we can conclude that the posterior distribution is:\n$$f(p|X=k)\\sim Beta(a+k, b+n-k)$$\n","date":"2025-05-14T15:10:48+08:00","image":"https://example.org/post/beta_distribution/logcabin_hu16862062976006335668.jpg","permalink":"https://example.org/post/beta_distribution/","title":"Beta distribution"},{"content":"Definition of expectation\rDefinition (Expectation of a discrete random variable)\nThe expected value of a discrete random variable $X$ whose distinct possible values are $x_1, x_2, \\ldots$ is defined by $$\\mathbb E[X] = \\sum_{i} x_i P(X = x_i) = \\sum_{i} x_i f_X(x_i)$$ where $f_X(x)$ is the probability mass function of $X$.\nThe expected value of a continuous random variable $X$ is defined by $$\\mathbb E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx$$ where $f_X(x)$ is the probability density function of $X$.\nLaw of unconscious statistician (LOTUS)\rTheorem (LOTUS)\nIf $X$ is a discrete random variable, and $g$ is a function from $\\mathbb R$ to $\\mathbb R$. If $Y=g(X)$, then $$\\mathbb E[Y] = \\mathbb E[g(X)] = \\sum_{x} g(x) P(X = x) = \\sum_{x} g(x) f_X(x)$$ where $f_X(x)$ is the probability mass function (PMF) of $X$.\nIf $X$ is a continuous random variable, and $g$ is a function from $\\mathbb R$ to $\\mathbb R$. If $Y=g(X)$, then $$\\mathbb E[Y] = \\mathbb E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx$$ where $f_X(x)$ is the probability density function (PDF) of $X$.\nWhy LOTUS?\rExample 1\nLet:\n$X \\sim Exponential(\\lambda)$ $Y=log(X)$ If we use the definition of expectation to calculate $\\mathbb E[Y]$, we can first calculate the PDF of $Y$: $$f_Y(y) = \\frac{d}{dy} P(Y \\leq y) = \\frac{d}{dy} P(X \\leq e^y) = \\frac{d}{dy} (1 - e^{-\\lambda e^y})$$ $$= \\lambda e^{-\\lambda e^y} e^y = \\lambda e^{-\\lambda e^y + y}$$\nThen we can calculate $\\mathbb E[Y]$: $$\\mathbb E[Y] = \\int_{-\\infty}^{\\infty} y f_Y(y) dy = \\int_{-\\infty}^{\\infty} y \\lambda e^{-\\lambda e^y + y} dy$$\nHowever, LOTUS could make it easier: $$\\mathbb E[Y] = \\mathbb E[log(X)] = \\int_{0}^{\\infty} log(x) \\lambda e^{-\\lambda x} dx$$\nExample 2\nLet:\n$X \\sim N(0,1)$ $Y = sin(X)$ If we use the definition of expectation to calculate $\\mathbb E[Y]$, we can first calculate the PDF of $Y$: $$f_Y(y) = \\frac{d}{dy} P(Y \\leq y) = \\frac{d}{dy} P(X \\leq sin^{-1}(y))$$ However, there’s no closed-form PDF for $Y$ as we cannot write the analytic form of $sin^{-1}(y)$. It’s a complicated, oscillatory transformation of a normal variable. You can’t write down $f_Y(y)$ analytically. Consequently, we cannot calculate $\\mathbb E[Y]$ using the definition of expectation.\nHowever, we can use LOTUS: $$\\mathbb E[Y] = \\mathbb E[sin(X)] = \\int_{-\\infty}^{\\infty} sin(x) f_X(x) dx = \\int_{-\\infty}^{\\infty} sin(x) \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} dx$$ This is a analytically solvable integral. The result is $0$ because the integrand is an odd function and the limits are symmetric about $0$.\nIn summary\rLOTUS is a powerful tool for calculating the expected value of a function of a random variable. It allows us to avoid the need to find the PDF of the transformed variable. It can simplify the calculation of expected values for complex transformations. It is applicable to both discrete and continuous random variables. ","date":"2025-05-14T15:10:48+08:00","image":"https://example.org/post/lotus/windmill_hu16029235293494346924.png","permalink":"https://example.org/post/lotus/","title":"LOTUS"},{"content":"Theorem (Change of variables in one dimension) Let $X$ be a random variable with PDF $f_X$, and let $Y=g(X)$, where $g$ is differentiable and monotonic. Then the PDF of $Y$ is given by: $$f_Y(y)=f_X(x)|\\frac{dx}{dy}|$$ where $x=g^-1(y)$. The support of Y is all $g(x)$ with $x$ in the support of $X$.\nProof.\nSuppose g is strictly increasing. Then, for $y=g(x)$, the CDF of $Y$ is: $$F_Y(y)=P(Y\\leq y)=P(g(X)\\leq y)=P(X\\leq g^-1(y))=F_X(g^-1(y))=F_X(x)$$ Therefore, the PDF of $Y$ is $$f_Y(y)=\\frac{d}{dy}F_Y(y)=\\frac{d}{dy}F_X(x)=\\frac{dx}{dy}\\frac{d}{dx}F_X(x)=f_X(x)\\frac{dx}{dy}$$\nSuppose g is strictly decreasing. Then, for $y=g(x)$, the CDF of $Y$ is: $$F_Y(y)=P(Y\\leq y)=P(g(X)\\leq y)=P(X\\geq g^-1(y))=1-P(X\\leq g^-1(y))=1-F_X(g^-1(y))=1-F_X(x)$$ Therefore, the PDF of $Y$ is $$f_Y(y)=\\frac{d}{dy}F_Y(y)=-\\frac{d}{dy}F_X(x)=-\\frac{dx}{dy}\\frac{d}{dx}F_X(x)=-f_X(x)\\frac{dx}{dy}$$ In this case, the $\\frac{dx}{dy}$ is negative, so we can write: $$f_Y(y)=f_X(x)|\\frac{dx}{dy}|$$ Combine the two cases: $$f_Y(y)=f_X(x)|\\frac{dx}{dy}|$$ where $x=g^-1(y)$. The support of Y is all $g(x)$ with $x$ in the support of $X$. ","date":"2025-05-14T14:35:45+08:00","image":"https://example.org/post/change_of_variables/House_hu2192646497281320324.png","permalink":"https://example.org/post/change_of_variables/","title":"Change_of_variables"},{"content":"Taylor Expansion / Series\rIn mathematics, the Taylor expansion / series of a function is an infinite sum of terms that are expressed in terms of the function\u0026rsquo;s derivatives at a single point. For most common functions, the function and the sum of its Taylor series are equal near this point.\nSuppose $f(x)$ is a real or composite function, which is a differentiable function of a neighborhood of the point $a$. The Taylor series of $f(x)$ about the point $a$ is given by:\n$$f(x)=f(a)+\\frac{f^{\\prime}(a)}{1!}(x-a)+\\frac{f^{\\prime\\prime}(a)}{2!}(x-a)^2+\\frac{f^{\\prime\\prime\\prime}(a)}{3!}(x-a)^3+\\dots$$\nWhere $n!$ denotes the factorial of $n$. In the more compact sigma notation, this can be written as: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(a)}{n!}(x-a)^n$$\nExample: the power series for $e^x$ is: $$e^x=\\sum_{n=0}^\\infty \\frac{x^n}{n!}$$\nProof.\nAs the derivative of $e^x$ is itself: $$f^{(n)}(x)=e^x$$\nTaking the derivative at $x=0$: $$f^{(n)}(0)=e^0=1$$\nUsing Taylor expansion, set $a=0$: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(a)}{n!}(x-a)^n=\\sum_{n=0}^\\infin\\frac{x^n}{n!}$$\nIn summary\nTaylor expansion is a way to approximate a function using infinitely many polynomial terms.\nPower Series\rA power series is an infinite series of the form: $$\\sum_{n=0}^\\infty a_n(x-a)^n$$\nWhere:\n$a_n$ are the coefficients of the series $x$ is the variable $a$ is a constant. Example: the power series for $sin(x)$ is: $$sin(x)=\\sum_{n=0}^\\infin\\frac{(-1)^nx^{2n+1}}{(2n+1)!}=x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\frac{x^7}{7!}+\\frac{x^9}{9!}-\\frac{x^11}{11!}+\\dots$$\nProof.\nUsing the Taylor expansion with $a=0$: $$f(x)=sin(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(0)}{n!}(x)^n$$\nCompute the derivatives of $sin(x)$ at $x=0$: $$f(0)=sin(0)=0$$ $$f^{(1)}(0)=cos(0)=1$$ $$f^{(2)}(0)=-sin(0)=0$$ $$f^{(3)}(0)=-cos(0)=-1$$ $$f^{(4)}(0)=sin(0)=0$$ $$f^{(5)}(0)=cos(0)=1$$ $$f^{(6)}(0)=-sin(0)=0$$ $$f^{(7)}(0)=-cos(0)=-1$$ $$\\dots$$ You can see that only the odd derivatives are non-zero, and they alternate in sign.\nPlug into the Taylor expansion: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(0)}{n!}(x)^n=\\sum_{n=0}^\\infin\\frac{(-1)^nx^{2n+1}}{(2n+1)!}$$\nIn Summary\nEvery Taylor series is a power series, but not every power series is a Taylor series. Power series serve as a broad framework, while Taylor series are tailored (pun intended!) to approximate specific functions using their derivatives. The convergence of a Taylor series to its function depends on the function being analytic (i.e., equal to its Taylor series in some neighborhood). ","date":"2025-05-13T20:14:20+08:00","image":"https://example.org/post/taylor_expansion/Figure_hu7592963123002402754.jpg","permalink":"https://example.org/post/taylor_expansion/","title":"Taylor Expansion / Series and Power Series"},{"content":"Moments in Probability\rFor a random variable $X$, The $n$-th moments of $X$ is the expectation of $X^n$, which is $\\mathbb{E}[X^n]$.\nWhy we need moment? Let’s starts with some measures we often use to describe a distribution.\nMean: $\\mu=\\mathbb{E}[X]$. Mean measures the central tendency. It tells us something about the center of a distribution. It is the $1$-th moment of $X$.\nVariance: $\\sigma=\\mathbb{E}[X-\\mu]^2$, Variance measures of dispersion. It is a measure of how far a set of numbers is spread out from their average value. It is the $2$-nd moment of $X-\\mu$.\nSkewness: $Skew(X)=\\mathbb{E}[\\frac{X-\\mu}{\\sigma}]^3$. Skewness measures the level of asymmetry level of a distribution. It is the $3$-rd moment of standardized $X$.\nKurtosis:: $Kurt(X)=\\mathbb{E}[\\frac{X-μ}{σ}]^4-3$. Kurtosis measures the tail’s heavy level of a distribution. It is the $4$-th moment of standardized $X$. The reason for subtracting 3 is to make any Normal distribution have kurtosis of 0.\nAs we can see from the example, moment could be used to measure the characteristics of distributions.\nSample moments\rIn previous section, we used some examples to introduce the moment, and why we need it. We used $\\mathbb{E}[X]$, which is the $1$-st moment of $X$, to represent the mean. Back to primary school, we were taught to use $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nx_i$ to calculate the mean of a observed data. For example, if we observe a list of $[1,3,4,2,2,6]$, the mean of this list would be $(1+3+4+2+2+6)/6=3$. Here is the question, what is the difference between $\\mathbb{E}[X]$ and $\\bar{X}$?\nIn probability, we use $\\mathbb{E}[X]$ to represent the mean of a random variable $X$. In statistics, we use $\\bar{X}$ to represent the mean of a observed data (sampled data, also called sample). $\\mathbb{E}[X]$ is the population mean of a random variable. Let\u0026rsquo;s say $X \\backsim N(0,1)$, then the $\\mathbb{E}[X]$ is equal to $0$. While $\\bar{X}$ is the sample mean of a observed data. In the real situation, we can only observe a sample of $X$, not the whole population. Therefore, the $\\bar{X}$ calculated from $\\frac{1}{n}\\sum_{i=1}^nx_i$ might not eactly equal to $0$. As the sample mean $\\bar{X}$ is an estimate of the population mean $\\mathbb{E}[X]$.\nSimilarly, the sample variance is defined as: $$ S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2 $$ The sample variance $S_n^2$ is an estimate of the population variance $\\sigma^2$. The sample variance is an unbiased estimator of the population variance. The reason for using $n-1$ instead of $n$ is to make the sample variance an unbiased estimator of the population variance. If we use $n$, then the sample variance would be biased.\nSimilarly, we can define the sample skewness as: $$ S_n^3=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_i-\\bar{x}}{S_n})^3 $$ and the sample kurtosis as: $$ S_n^4=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_i-\\bar{x}}{S_n})^4-3 $$\nMoment generating functions\rTheorem (Moment via derivatives of the MGF). The moment generating function (MGF) of a random variable $X$ is defined as: $$ M_X(t)=\\mathbb{E}[e^{tX}] $$ Given the MGF of $X$, we can get the $n$-th moment of $X$ by taking the $n$-th derivative of the MGF and evaluating it at $t=0$: $$\\mathbb{E}[X^n]=M^{(n)}(0)$$\nProof.\nUsing Taylor expansion of $M_X(t)$ around $t=0$: $$M_X(t)=\\mathbb{E}[e^{tX}]=\\sum_{n=0}^\\infin M^{(0)}(0)\\frac{t^n}{n!} \\tag{1}$$\nUsing Power series expansion of $e^{tX}$: $$e^{tX}=\\sum_{n=0}^\\infin \\frac{(tX)^n}{n!}=\\sum_{n=0}^\\infin \\frac{t^nX^n}{n!}$$\nSo the expectation of $e^{tX}$ is: $$\\mathbb{E}[e^{tX}]=\\sum_{n=0}^\\infin \\frac{t^n}{n!}\\mathbb{E}[X^n] \\tag{2}$$\nComparing (1) and (2), we can get: $$\\mathbb{E}[X^n]=M^{(n)}(0)$$\nThis theorem is suprising in that for a continuous random variable $X$, to compute moments would seemingly require doing integrals with LOTUS, but with the MGF, it is possible to find moments by taking derivatives rather than doing integrals.\n","date":"2025-05-13T00:53:25+08:00","image":"https://example.org/post/moments/Minecraft_DH_hu9496421566315252534.png","permalink":"https://example.org/post/moments/","title":"Moments in Probability"}]